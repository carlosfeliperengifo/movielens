---
title: "Movie Lens"
author: "Carlos Felipe Rengifo Rodas"
date: '2022-04-04'
output: bookdown::pdf_document2
---

```{r, echo=FALSE, include=FALSE}
library(tidyverse)
library(caret)
library(data.table)
knitr::opts_chunk$set(echo = TRUE)

# Set the variable "create_data_set" to 1 to download and rebuild the data 
# set, otherwise a "movielens" data frame previously built will be loaded from 
# the "movielens.Rda" file.
create_data_set <- 1
# Create the "movielens" data set
if (create_data_set) {
  dl <- tempfile()
  download.file("https://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

  # The "ratings" data frame comprises the columns "userId", "movieId", "rating", 
  # and "timestamp"
  ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))
  
  # The "movies" data frame comprises the columns "movieId", "title" and "genre"
  movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
  colnames(movies) <- c("movieId", "title", "genres")
  movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(movieId),
                                           title = as.character(title),
                                           genres = as.character(genres))

  # The "movielens" data frame cmbines the data frames "ratings" and "movies", 
  # therefore its columns are "userId", "movieId", "rating", "timestamp",
  # "title" and "genre".
  movielens <- left_join(ratings, movies, by = "movieId")
  save(movielens, file = "movielens.Rda")
} else {
  load("movielens.Rda")
}
# Number of movies, users, and genres
nmovies <- unique(movielens$movieId) %>% length()
nusers <- unique(movielens$userId) %>% length()
ngenres <- unique(movielens$genres) %>% length()
```

<!-- 
An introduction/overview/executive summary section that describes the data set 
and summarizes the goal of the project and key steps that were performed.
-->

# Introduction

The MovieLens dataset used in this project comprises 10 millions ratings for 
`r nmovies` movies rated by `r nusers` users. The scale for the rating goes from
0 to 5 with increments of 0.5. The product between the number of users and 
the number of movies is close to `r round(nusers*nmovies/1E6)` millions, which 
is greater than the number of ratings. This difference indicates that not every 
user rated every movie. The MovieLens dataset has six columns which are: (1) the 
movie identifier, (2) the user identifier, (3) the rating of the movie, (4) a 
time stamp with the date of the rating, (5) the title of the movie, and (6) the 
genres of the movie.

The purpose of this project is to build a linear model to predict the rating 
$r_{m,u}$ that the user $u$ will give to the movie $m$. Such a model can be 
mathematically written as follows:

$$
r_{m,u} = \mu + \lambda\, \left(b_m + b_u\right) + \epsilon_{m,u}
$$

$\mu$ is the average movie rating, $b_m$ represents the movie-to-movie variation, 
$b_u$ is the user-to-user variation, $\epsilon_{m,u}$ is a zero mean 
random variable representing uncertainty, and $\lambda$ is a penalty
factor that avoids large values for $b_m$ and $b_u$. A second model that we want
to explore includes the movie genre effect $b_g$:

$$
r_{m,u,g} = \mu + \lambda\, \left(b_m + b_u + b_g\right) + \epsilon_{m,u,g}
$$


<!-- 
a methods/analysis section that explains the process and techniques used, 
including data cleaning, data exploration and visualization, insights gained, 
and your modeling approach
-->

# Methods

## Data exploration and visualization

The Table \@ref(tab:mostrated) presents the titles of the most rated movies. The 
Figure \@ref(fig:hmratings) shows a histogram with the distribution of ratings 
among movies, and the Figure \@ref(fig:huratings) shows a histogram with the 
distribution of ratings among users.

```{r mostrated, echo=FALSE }
rmovies <- movielens %>% group_by(movieId) %>% 
  summarise(NRatings = n()) 
topId <- rmovies %>% arrange(desc(NRatings)) %>% head()
topId %>% left_join(movielens,by="movieId") %>% group_by(movieId) %>%
  slice(1) %>% ungroup() %>% select(title, NRatings) %>% 
  arrange(desc(NRatings)) %>% 
  rename("Title"=title,"Num. of ratings"=NRatings) %>%
  knitr::kable(caption = "Most rated movies")
```

```{r hmratings, echo=FALSE, out.width="50%", fig.align='center', fig.cap ="Distribution of ratings among movies"}

rmovies %>% ggplot(aes(x = NRatings)) + geom_histogram(bins = 10) +
    scale_x_continuous(trans='log10')
```

```{r huratings, echo=FALSE, out.width="50%", fig.align='center', fig.cap ="Distribution of ratings among users"}
umovies <- movielens %>% group_by(userId) %>% 
  summarise(NRatings = n()) %>% arrange(desc(NRatings))
umovies %>% ggplot(aes(x = NRatings)) + geom_histogram(bins = 10) +
    scale_x_continuous(trans='log10')
```

## Insights gained

The data exploration and visualization of the previous section permit us to
conclude that not every movie received the same number of ratings, and that no 
every user rated the same number of movies.

## Modeling approach

The approach followed in this report consists of building models of increasing complexity until a mean square error of less than $0.86490$ is obtained. These 
are the models that will be used:

1. $r_{m,u} = \mu + \epsilon_{m,u}$.
2. $r_{m,u} = \mu + b_m + \epsilon_{m,u}$.
3. $r_{m,u} = \mu + \lambda\, b_m + \epsilon_{m,u}$.
4. $r_{m,u} = \mu + \lambda\, \left(b_m + b_u\right) + \epsilon_{m,u}$
5. $r_{m,u,g} = \mu + \lambda\, \left(b_m + b_u + b_g\right) + \epsilon_{m,u,g}$

The quality of a model will be assessed according to the root mean squared error,
which is defined as follows:

$$
RMSE = \sqrt{\sum_{k=1}^N \left(r_{m,u} - \hat{r}_{m,u}\right)^2}
$$
$r_{m,u}$ is the actual rating given to movie $m$ by the user $u$, and 
$\hat{r}_{m,u}$ is the prediction of such rating according to one of
the models previously described. The lower the RMSE, the higher the accuracy
of a model.

```{r, echo=FALSE, include=FALSE}
# Function to calculate the root mean square error
RMSE <- function(true_ratings, predicted_ratings){
  sqrt(mean((true_ratings - predicted_ratings)^2))
}
```

## Data cleansing

### Download the MovieLens database

The first step to build the dataset is to download the file *ml-10m.zip* from the 
[\textcolor{blue}{MovieLens}](https://files.grouplens.org/datasets/movielens/ml-10m.zip) website. *ml-10m.zip* contains the files *ratings.dat* and *movies.dat*. The rows
of the first file comprise a user identifier, a movie identifier, a rating, and 
a time stamp; while the rows of the file are composed by a movie identifier, a 
movie title, and a movie genre. The information obtained from *ratings.dat* 
and *movies.dat* is combined into a single data frame named *movieLens*.

### Data partition

The second step is to use the function *createDataPartition* of the *caret* 
package to divide the data set into $90\%$ for modelling and $10\%$ for
validation. The modelling and validation data frames will be termed *edx* and
*validation*, respectively.

```{r, echo=FALSE, include=FALSE}
# The first step is to use 90% of the data for training, and 10 % for 
# validation.
set.seed(1, sample.kind="Rounding") 
test_index <- createDataPartition(y = movielens$rating, times = 1, 
                                  p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Filtering joins filter rows from x based on the presence or absence of matches
# in y:
# semi_join(x, y) return all rows from x with a match in y.
# anti_join(x, y) return all rows from x without a match in y.

# Ensures that all users and movies in the "validation" data frame are also in 
# "edx" data frame
validation <- temp %>% semi_join(edx, by = "movieId") %>% 
  semi_join(edx, by = "userId")

# Puts the rows removed from the "validation" data frame into the "edx" data 
# frame.
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)
```

# Results

## Estimation of the population mean

```{r umean, echo=FALSE, include=FALSE}
mu <- mean(edx$rating)
yhat1 <- rep(mu, nrow(validation))
rmse1 <- RMSE(validation$rating, yhat1)
rmse_row1 <- data.frame(Method = "u+e", RMSE = rmse1)
```

The population mean is estimated by averaging all ratings of the *edx* data 
frame. The resulting value is $\hat{u} =$ `r round(mu,2)`, which means that no
matter the user or the movie, the predicted rating is:

$$
\hat{r}_{m,u} = `r round(mu,2)`
$$
The RMSE for this model is `r round(rmse1,4)`, which means that in average our
predictions differ in more than one star from the actual rating.

## Estimation of the movie effect

```{r model2, echo=FALSE, include=FALSE}
# Movie effect
movie_avgs <- edx %>% group_by(movieId) %>% summarize(bi = mean(rating - mu))
# Predicted rating
yhat2 <- validation %>% left_join(movie_avgs, by="movieId") %>% 
          mutate(yhat = mu + bi) %>% pull(yhat)
# RMSE
rmse2 <- RMSE(validation$rating, yhat2)
# Add RMSE to the data frame with the summary of results
rmse_row2 <- data.frame(Method = "u+bm+e", RMSE = rmse2)
```

The following model accounts for the movie effect:

$$
r_{m,u} = \mu + b_m + \epsilon_{m,u}
$$
$b_m$, which represent the movie bias, is estimated by averaging the ratings for
the movie $m$, and then subtracting the overall mean $\mu$. Since there are
`r nmovies` movies, the above model requires to estimate `r nmovies` bias (i.e.
$m = 1,2,\dots `r nmovies`$)

$$
\hat{b}_m = \frac{1}{N_m}\sum_{u\, \in\, u(m)} r_{u,m}-\mu
$$

$N_m$ is the number of users who rated the movie $m$, and $u(m)$ is the set of
users who rated such movie. The RMSE for this model is `r round(rmse2,4)`. However,
some values of $\hat{b}_m$ are not reliable because they were estimated using just 
a few ratings (one or two in some cases). The Table \@ref(tab:bestrated) shows 
that the best rated movies have only one or two ratings.

```{r bestrated, echo=FALSE}
# Best rated movies
best_rated <- edx %>% group_by(movieId) %>% 
  summarise(avrating = mean(rating), nratings = n()) %>% 
  arrange(desc(avrating)) %>% head()
best_rated %>% left_join(edx,by="movieId") %>% 
  select(title,avrating,nratings) %>%  rename("Title"=title) %>%
  rename("Av. Rating"=avrating, "Num. Ratings"=nratings) %>%
  knitr::kable(caption = "Movies with the highest average ratings")
```

## Estimation of the movie effect with regularization

Regularization permit us to take into consideration that the values of 
$\hat{b}_m$ estimated with very few ratings are random variables with higher 
variability than the $\hat{b}_m$ estimated with hundreds or thousands of 
ratings. In a model with regularization, a penalty factor $\lambda$ 
multiplies the estimate that is regularized:

$$
r_{u,m} = \mu + \lambda\, b_m + \epsilon_{u,m}
$$

The regularized estimate, $b_m$ in the present example, is estimated as follows:

$$
\hat{b}_m = \frac{1}{N_u + \lambda}\sum_{u\, \in\, u(m)} r_{u,m} - \mu
$$

```{r regb1, echo=FALSE, include=FALSE}
lambdas <- seq(0, 10, 0.25)
#lambdas <- 5.25 
rmses <- sapply(lambdas, function(l){
  # Movie effect
  bi <- edx %>% group_by(movieId) %>% summarize(bi = sum(rating - mu)/(n()+l))
  # Prediction
  yhat <- validation %>% left_join(bi, by = "movieId") %>%
            mutate(pred = mu + bi) %>% pull(pred)
  # Return
  return(RMSE(yhat, validation$rating))
})
#qplot(lambdas, rmses)
lmin_bm <- lambdas[which.min(rmses)]
rmse3 <- min(rmses)
rmse_row3 <- data.frame(Method = "u+lambda*bm+e", RMSE = rmse3)
```

When $\lambda$ is varied from $0$ to $10$ with increments of $0.25$, the value 
that minimizes the RMSE on the training set is $\lambda =$ `r round(lmin_bm,2)`,
and the corresponding RMSE is `r round(rmse3,4)`. This RMSE is lower than the
obtained from the previous model.

## Estimation of the movie and user effects with regularization

The following model accounts for both the movie and the user effect:

$$
r_{u,m} = \mu + \lambda(b_m + b_u) + \epsilon_{u,m}
$$

$b_u$, which represent the user bias, is estimated by averaging the ratings 
given by user $u$, and then subtracting $\mu + b_m$. Since there are
`r nusers` users, the above model requires to estimate `r nusers` bias (i.e.
$u = 1,2,\dots `r nusers`$)

$$
\hat{b}_u = \frac{1}{N_u+\lambda}\sum_{m\, \in\, m(u)} r_{m,u}-\mu-b_m
$$
```{r model4, echo=FALSE, include=FALSE}
lambdas <- seq(0, 10, 0.25)
#lambdas <- 5.25 
rmses <- sapply(lambdas, function(l){
  # Movie effect
  bi <- edx %>% group_by(movieId) %>% summarize(bi = sum(rating - mu)/(n()+l))
  # User effect
  bu <- edx %>% left_join(bi, by="movieId") %>% group_by(userId) %>%
           summarize(bu = sum(rating - bi - mu)/(n()+l))
  # Prediction
  yhat <- validation %>% left_join(bi, by = "movieId") %>%
    left_join(bu, by = "userId") %>% mutate(pred = mu + bi + bu) %>% pull(pred)
  # Return
  return(RMSE(yhat, validation$rating))
})
#qplot(lambdas, rmses)
lmin_bmbu <- lambdas[which.min(rmses)]
rmse4 <- min(rmses)
rmse_row4 <- data.frame(Method = "u+lambda*(bm+bu)+e", RMSE = rmse4)
```

$N_u$ is the number of movies rated by the user $u$, and $m(u)$ is the set of 
movies rated by the user $u$. When $\lambda$ is varied from $0$ to $10$ with 
increments of $0.25$, the value that minimizes the RMSE on the training set is
$\lambda =$ `r round(lmin_bmbu,2)`, and the corresponding RMSE is 
`r round(rmse4,4)`. 

## Estimation of the movie, user and genre effects with regularization

The following includes the movie, user and genre effects:

$$
r_{u,m,g} = \mu + \lambda(b_m + b_u + b_g) + \epsilon_{u,m,g}
$$

$b_g$, which represent the genere bias, is estimated by averaging the ratings 
given to the movies of genre $g$, and then subtracting $\mu + b_m + b_u$. Since
there are `r ngenres` genres, the above model requires to estimate `r ngenres` 
bias (i.e. $g = 1,2,\dots `r ngenres`$)

```{r model5, echo=FALSE, include=FALSE}
lambdas <- seq(0, 10, 0.25)
#lambdas <- 5.25 
rmses <- sapply(lambdas, function(l){
  # Movie effect
  bm <- edx %>% group_by(movieId) %>% summarize(bm = sum(rating - mu)/(n()+l))
  # User effect
  bu <- edx %>% left_join(bm, by="movieId") %>% group_by(userId) %>%
           summarize(bu = sum(rating - bm - mu)/(n()+l))
  # Genre effect
  bg <- edx %>% left_join(bm, by="movieId") %>% left_join(bu, by="userId") %>%
    group_by(genres) %>% summarize(bg = sum(rating - bm - bu - mu)/(n()+l))
  # Prediction
  yhat <- validation %>% left_join(bm, by="movieId") %>%
    left_join(bu, by="userId") %>% left_join(bg, by="genres") %>%  
    mutate(pred = mu + bm + bu + bg) %>% pull(pred)
  # Return
  return(RMSE(yhat, validation$rating))
})
#qplot(lambdas, rmses)
lmin_bmbubg <- lambdas[which.min(rmses)]
rmse5 <- min(rmses)
rmse_row5 <- data.frame(Method = "u+lambda*(bm+bu+bg)+e", RMSE = rmse5)
```

$$
\hat{b}_g = \frac{1}{N_g+\lambda}\sum_{m\, \in\, m(g)}
\left[\sum_{u\, \in\, u(g)} r_{m,u,g}-\mu-b_m-b_u\right]
$$
$N_g$ is the number of movies of genre $g$, $m(g)$ is the set of 
movies of genre $g$, $u(g)$ is set of users who rated movies of genre $g$. When $\lambda$ is varied from $0$ to $10$ with increments of $0.25$, the value of 
$\lambda$ that minimizes the RMSE on the training set is 
$\lambda =$ `r round(lmin_bmbubg,2)`, and the corresponding RMSE is 
`r round(rmse5,2)`. 

## Summary of results

The Table \@ref(tab:summary) summarizes the RMSE obtained from each model.

```{r summary, echo=FALSE}
# RMSE by model
rmse_results <- rbind(rmse_row1, rmse_row2, rmse_row3, rmse_row4, rmse_row5)
rmse_results %>% knitr::kable(caption = "RMSE by model")

```

# Conclusion

<!-- 
A conclusion section that gives a brief summary of the report, its limitations and future work
-->

This report presented five models to predict the rating that a user gives to
a movie. The complexity of the models is increasing. The first model considered 
only the overall mean rating, the second model comprised the overall mean rating 
and the movie effect, and the third model included the overall mean rating and the 
movie effect with regularization, which is necessary to avoid large values of the 
movie bias generated by movies with very few ratings. The fourth model 
considered the overall mean and regularized terms for the movie and the user 
effect. The five model was an extension of the fourth model that considered the 
effect of the movie genre in the rating. However, to include the genre effect 
decreased the RMSE from `r round(rmse4,4)` to just `r round(rmse5,4)`. The main limitation of this report is that important predictors such as the product 
between user and genre of the movie were not included in the model. Another 
limitation of our study is that the penalty factors for the effect of user, 
movie and genre were the same. Future works should consider to include products 
between variables and apply multivariable optimization to obtain three 
regularization factors instead of one. We also hypothesized that including users' gender, geographic location, native language, and date of rating in the 
prediction model could improve accuracy.


